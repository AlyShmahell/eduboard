\documentclass{beamer}
\usetheme{metropolis}
\setbeamercolor{background canvas}{bg=white}
\usepackage{graphicx}
\graphicspath{{../dissertation/graphics/}}
\usepackage{natbib}
\usepackage{lmodern}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{colortbl} 
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newtcolorbox[blend into=tables]{colortable}[2][]{
	colback=white,
	tabularx*={\renewcommand{\arraystretch}{1.0}}{Y|Y|Y|Y|Y|Y},title={#2},boxrule=0.8pt, center title
}

\title{Hyper Heuristic Cryptography with Mixed Adversarial Nets}
\date{\today}
\author{\textbf{Author:} Aly Shmahell \\ \textbf{Supervisor:} Prof. Giovanni De Gasperis}
\institute{University of L'Aquila}
\begin{document}
	\maketitle
	\section{Introduction}
		\begin{frame}{What This Thesis Is About}
			\vfill
			\textbf{Neural Cryptography:} \\\vfill
			\hfill\begin{minipage}{0.93\textwidth}
				applying stochastic methods to get neural nets to achieve cryptographic functionality.
				\end{minipage}
			\vfill
			\textbf{Basis For This Thesis:}\\\vfill
			\hfill\begin{minipage}{0.93\textwidth}
				a recent paper released in 2016 from Google Brain~\citep{DBLP:journals/corr/AbadiA16}.
			\end{minipage}
			\vfill
			\textbf{How The Thesis Extends Its Basis:}\\\vfill
			\hfill\begin{minipage}{0.93\textwidth}
				by focusing on increasing confidentiality of communication, while minimizing loss of information integrity.
			\end{minipage}
		\end{frame}
		\begin{frame}{Basic Cryptology Scheme}
			\vfill
			\begin{center}
				\includegraphics[height=0.8\textheight]{Alice-Bob-Eve-Alan}
			\end{center}
			\vfill
		\end{frame}
		\begin{frame}{Symmetric Results From Google Brain - For Comparison}
			\begin{center}
				\includegraphics[height=0.8\textheight]{cryptolearn_batch_tighter}
			\end{center}
		\end{frame}
		\begin{frame}{Asymmetric Results From Google Brain - For Comparison}
			\begin{center}
				\includegraphics[height=0.8\textheight]{pubkey_bob_v_eve}
			\end{center}
		\end{frame}
		\begin{frame}{Justification For Neural Cryptography}
			 \vfill
			 \textbf{Neural Cryptography Is Viable:}\\\vfill
			 \hfill\begin{minipage}{0.93\textwidth}
			 	convolutional nets can construct local spatial relations in data.
			 \end{minipage}
			 \vfill
			 \textbf{Neural Cryptanalysis Is Viable:}\\\vfill
			 \hfill\begin{minipage}{0.93\textwidth}
			 	fully connected layers can detect global spatial relations in data.
			 \end{minipage}
			 \vfill
			 \textbf{Neural Cryptography Can Be Fast:}\\\vfill
			 \hfill\begin{minipage}{0.93\textwidth}
			 	convolutional nets share weights using their filters.
			 \end{minipage}
			 \vfill
			 \textbf{Neural Cryptography Is Evolved, Not Patched:}\\\vfill
			 \hfill\begin{minipage}{0.93\textwidth}
			 	using adversary in training evolves weights which serves to tweak the cryptographic functionality.
			 \end{minipage}
		\end{frame}
		\begin{frame}{What This Thesis Adds To The Research Pool}
			\vfill
			\textbf{A Prototype Blueprint:}\\\vspace{0.1cm}
			\hfill\begin{minipage}{0.93\textwidth}
				for a software-engineered neural crypto-system.
			\end{minipage}
			\vfill
			\textbf{An Analysis Of How Neural Components Work:}\\\vfill
			\hfill\begin{minipage}{0.93\textwidth}
				when the objective is to achieve cryptographic functionality.
			\end{minipage}
			\vfill
			\textbf{An Enhancement In The Neural Structures:}\\\vfill
			\hfill\begin{minipage}{0.93\textwidth}
				which yields a boost in cryptographic robustness.
			\end{minipage}
			\vfill
			\textbf{Transfer Learning:}\\\vfill
			\hfill\begin{minipage}{0.93\textwidth}
				to get symmetric neural cryptography on par with asymmetric neural cryptography.
			\end{minipage}
		\end{frame}
		\section{Experiments \& Results}
		\begin{frame}{Symmetric Scheme}
			\begin{center}
				\includegraphics[width=\textwidth, height=0.8\textheight]{symmetricScheme-present}
			\end{center}
		\end{frame}
		\begin{frame}{Thesis Results - Symmetric Training}
			\begin{center}
				\includegraphics[height=0.9\textheight]{neurencoder-symmetric-training}
			\end{center}
		\end{frame}
		\begin{frame}{Thesis Results - Symmetric Testing}
			\begin{center}
				\includegraphics[height=0.9\textheight]{neurencoder-symmetric-testing}
			\end{center}
		\end{frame}
		\begin{frame}{Asymmetric Scheme}
			\begin{center}
				\includegraphics[width=\textwidth, height=0.8\textheight]{asymmtericScheme-present}
			\end{center}
		\end{frame}
		\begin{frame}{Thesis Results - Asymmetric Training}
			\begin{center}
				\includegraphics[height=0.9\textheight]{neurencoder-asymmetric-training}
			\end{center}
		\end{frame}
		\begin{frame}{Thesis Results - Asymmetric Testing}
			\begin{center}
				\includegraphics[height=0.9\textheight]{neurencoder-asymmetric-testing}
			\end{center}
		\end{frame}
		\begin{frame}{Transfer Learning}
			\textbf{Transfer Learning:}\\\vfill
			\hfill\begin{minipage}{0.93\textwidth}
				
			\end{minipage}
			{\centering
				Given a source domain $ D_{S} $ and learning task $ T_{S} $ , a target domain $ D_{T} $ and learning task
				$ T_{T} $ , transfer learning aims to help improve the learning of the
				target predictive function $ f_{T}(\cdot) $ in $ D_{T} $ using the knowledge in
				$ D_{S} $ and $ T_{S} $ , where $ D_{S} \neq D_{T} $ , or $ T_{S} \neq T_{T} $.~\citep{5288526}
				\par} 
		\end{frame}
		\begin{frame}{Hybrid Scheme}
			\begin{center}
				\includegraphics[width=\textwidth, height=0.8\textheight]{hybridScheme-present}
			\end{center}
		\end{frame}
		\begin{frame}{Thesis Results - Hybrid Training}
			\begin{center}
				\includegraphics[height=0.9\textheight]{neurencoder-hybrid-training}
			\end{center}
		\end{frame}
		\begin{frame}{Thesis Results - Hybrid Testing}
			\begin{center}
				\includegraphics[height=0.9\textheight]{neurencoder-hybrid-testing}
			\end{center}
		\end{frame}	
		\section{Implementation}
		\begin{frame}{Class Diagram}
			\begin{center}
				\includegraphics[height=0.9\textheight]{classDiagram-present}
			\end{center}
		\end{frame}	
	\section{Appendix:{\tiny } How I Chose My Activation Functions}
	\begin{frame}{Dummy Net Example}
		\begin{center}
			\includegraphics[height=0.9\textheight]{SimpleNetDiagram}
		\end{center}
	\end{frame}
		\begin{frame}{Activation Function Combinations}
			\textbf{Different Options:}
			\begin{itemize}
				\item $" Sigmoid \rightarrow LeakyRealu \rightarrow Sigmoid "$.
				\item $" Tanh \rightarrow LeakyRealu \rightarrow Tanh "$.
				\item $" Sigmoid \rightarrow LeakyRealu \rightarrow Tanh "$.
			\end{itemize}
			\textbf{The Empirically Reliable Choice:}\\
			\centering $" Sigmoid \rightarrow LeakyRealu \rightarrow Tanh "$.
		\end{frame}
		\begin{frame}{Activation Function Combinations - Numerical Analysis}
			$" Tanh \rightarrow LeakyRealu \rightarrow Tanh "$
			\begin{center}
				\includegraphics[height=0.88\textheight]{tanh_leakyRelu_tanh}
			\end{center}
		\end{frame}
		\begin{frame}{Activation Function Combinations - Numerical Analysis}
			$" Sigmoid \rightarrow LeakyRealu \rightarrow Sigmoid "$.
			\begin{center}
				\includegraphics[height=0.88\textheight]{sigmoid_leakyRelu_sigmoid}
			\end{center}
		\end{frame}
		\begin{frame}{Activation Function Combinations - Numerical Analysis}
			$" Sigmoid \rightarrow LeakyRealu \rightarrow Tanh "$.
			\begin{center}
				\includegraphics[height=0.88\textheight]{sigmoid_leakyRelu_tanh}
			\end{center}
		\end{frame}
		\begin{frame}
			\frametitle{References}
			\bibliographystyle{unsrt}
			\bibliography{../dissertation/latex_files/neurencoder.bib}
		\end{frame}
\end{document}
